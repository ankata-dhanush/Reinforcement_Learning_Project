

# Reinforcement Learning Project: SARSA Agent in Grid Environment

## **Project Overview**

This project implements a Reinforcement Learning agent using the **SARSA (State-Action-Reward-State-Action)** algorithm in a custom **Grid Environment**. The environment simulates a 4x4 grid with rewards and penalties associated with different positions such as **apple (+12)**, **banana (+15)**, **smoke (-10)**, **alcohol (-12)**, and reaching the **goal (+200)**. The RL agent learns to navigate through the grid to maximize cumulative rewards.

The project demonstrates the effects of different parameters like **discount factor (gamma)**, **maximum timesteps**, and **number of episodes** on agent performance.

---

## **Environment Details**

- **Grid size:** 4x4

- **States:** 16 (each grid cell is a state)

- **Actions:** 4 (0 = Down, 1 = Up, 2 = Right, 3 = Left)

- **Max timesteps per episode:** 12 (variable for experimentation)

- **Rewards:**

  - Apple: +12
  - Banana: +15
  - Smoke: -10
  - Alcohol: -12
  - Goal: +200

- **Observation:** Flattened 4x4 grid representing current agent, goal, and item positions.

The environment ensures that the agent stays within grid boundaries using clipping, and rewards are applied based on agent interaction with objects.

---

## **SARSA Algorithm**

The SARSA algorithm is an **on-policy RL algorithm** that updates its Q-values based on the action actually taken in the next state. The update rule is:

[
Q(s, a) \leftarrow Q(s, a) + \alpha \Big[ R(s,a) + \gamma Q(s', a') - Q(s, a) \Big]
]

**Hyperparameters used:**

- Learning rate ((\alpha)) = 0.3
- Discount factor ((\gamma)) = variable (tested 0.95, 0.7)
- Epsilon (exploration probability) = starts at 1, decays exponentially per episode
- Max episodes = variable (tested 600, 1000, 3000)

**Key features:**

- Epsilon-greedy action selection to balance exploration vs exploitation.
- Q-table representation for discrete states and actions.
- Tracks total rewards per episode and epsilon decay.

---

## **Agent Training and Evaluation**

1. **Training:** The agent is trained for a fixed number of episodes. Each episode starts from the initial agent position `[0,0]` and continues until either the goal is reached or maximum timesteps are exceeded.
2. **Q-table Update:** Q-values are updated at each step using SARSA update rule.
3. **Exploration vs Exploitation:** Epsilon decay ensures the agent gradually becomes greedy to leverage learned policy.
4. **Visualization:** The environment is rendered after each step to visualize agent movement.

---

## **Parameter Exploration**

The project experimented with different parameters:

| Parameter           | Values Tested   | Observations                                                                 |
| ------------------- | --------------- | ---------------------------------------------------------------------------- |
| Discount Factor (γ) | 0.95, 0.7       | Higher γ focuses on long-term rewards, lower γ focuses on short-term rewards |
| Max Timesteps       | 12              | Limiting steps affects agent's ability to reach goal                         |
| Number of Episodes  | 600, 1000, 3000 | More episodes improve convergence and policy stability                       |

---

## **Results & Graphs**

- **Total Rewards per Episode:** Shows how cumulative rewards improve as agent learns.
- **Epsilon Decay:** Demonstrates decreasing exploration over time as agent becomes greedy.

_Example of graphs generated by the SARSA agent:_

- Rewards curve rises as episodes increase, indicating improved policy.
- Epsilon curve decays exponentially, demonstrating reduced exploration.

---

## **How to Run the Project**

1. Install required packages:

```bash
pip install gym matplotlib
```

2. Import and initialize the environment:

```python
env = GridEnvironment()
env.reset()
```

3. Create and train SARSA agent:

```python
agent = SARSA(env)
agent.train(1000) # Number of episodes
```

4. Visualize agent performing learned policy:

```python
agent.Agent()
agent.graphs()
```

---

## **Conclusion**

- The SARSA agent successfully learns to navigate the grid to maximize rewards while avoiding penalties.
- Parameter tuning (gamma, episodes, max timesteps) plays a key role in policy effectiveness.
- The project provides a foundational implementation of **on-policy RL** in a discrete environment, suitable for further extensions like **Q-learning**, **Deep SARSA**, or **larger environments**.
